{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539aa8fa-3937-4713-a5d4-8e007e3d59c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/10 17:05:01 WARN Utils: Your hostname, arsa-IdeaPad-3-14ITL6, resolves to a loopback address: 127.0.1.1; using 192.168.1.206 instead (on interface wlp0s20f3)\n",
      "25/09/10 17:05:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/10 17:05:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+\n",
      "|EmployeeName|Department|Salary|\n",
      "+------------+----------+------+\n",
      "|       James|     Sales|  3000|\n",
      "|     Michael|     Sales|  4600|\n",
      "|      Robert|     Sales|  4100|\n",
      "|     Jamilah|Accounting|  3900|\n",
      "|      Michel|Accounting|  4600|\n",
      "|       Assep| Enggineer|  5000|\n",
      "|       Maria|   Finance|  3000|\n",
      "+------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# DataFrame sederhana dan operasi dasar\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('HandsOnPertemuan3').getOrCreate()\n",
    "\n",
    "data = [('James', 'Sales', 3000),\n",
    "        ('Michael', 'Sales', 4600),\n",
    "        ('Robert', 'Sales', 4100),\n",
    "        ('Jamilah', 'Accounting', 3900),\n",
    "        ('Michel', 'Accounting', 4600),\n",
    "        ('Assep', 'Enggineer', 5000),\n",
    "        ('Maria', 'Finance', 3000)]\n",
    "columns = ['EmployeeName', 'Department', 'Salary']\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fb1f7c-a738-4b65-b64b-f8822cf01770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|EmployeeName|Salary|\n",
      "+------------+------+\n",
      "|     Michael|  4600|\n",
      "|      Robert|  4100|\n",
      "|     Jamilah|  3900|\n",
      "|      Michel|  4600|\n",
      "|       Assep|  5000|\n",
      "+------------+------+\n",
      "\n",
      "+----------+------------+\n",
      "|Department|RataRataGaji|\n",
      "+----------+------------+\n",
      "|     Sales|      3900.0|\n",
      "|Accounting|      4250.0|\n",
      "|   Finance|      3000.0|\n",
      "| Enggineer|      5000.0|\n",
      "+----------+------------+\n",
      "\n",
      "+----------+-------------+\n",
      "|Department|GajiTertinggi|\n",
      "+----------+-------------+\n",
      "|     Sales|         4600|\n",
      "|Accounting|         4600|\n",
      "|   Finance|         3000|\n",
      "| Enggineer|         5000|\n",
      "+----------+-------------+\n",
      "\n",
      "+----------+------------+\n",
      "|Department|GajiTerendah|\n",
      "+----------+------------+\n",
      "|     Sales|        3000|\n",
      "|Accounting|        3900|\n",
      "|   Finance|        3000|\n",
      "| Enggineer|        5000|\n",
      "+----------+------------+\n",
      "\n",
      "+----------+---------+\n",
      "|Department|TotalGaji|\n",
      "+----------+---------+\n",
      "|     Sales|    11700|\n",
      "|Accounting|     8500|\n",
      "|   Finance|     3000|\n",
      "| Enggineer|     5000|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+--------------+\n",
      "|Department|JumlahKaryawan|\n",
      "+----------+--------------+\n",
      "|     Sales|             3|\n",
      "|Accounting|             2|\n",
      "|   Finance|             1|\n",
      "| Enggineer|             1|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operasi dasar DataFrame di PySpark\n",
    "\n",
    "# 1. Filter data karyawan dengan gaji di atas 3000 lalu tampilkan kolom nama dan gajinya\n",
    "df.filter(df['Salary'] > 3000).select('EmployeeName', 'Salary').show()\n",
    "\n",
    "# 2. Menghitung rata-rata gaji per departemen\n",
    "df.groupBy('Department').avg('Salary').withColumnRenamed(\"avg(Salary)\", \"RataRataGaji\").show()\n",
    "\n",
    "# 3. Menghitung gaji maksimum per departemen\n",
    "df.groupBy('Department').max('Salary').withColumnRenamed(\"max(Salary)\", \"GajiTertinggi\").show()\n",
    "\n",
    "# 4. Menghitung gaji minimum per departemen\n",
    "df.groupBy('Department').min('Salary').withColumnRenamed(\"min(Salary)\", \"GajiTerendah\").show()\n",
    "\n",
    "# 5. Menghitung total gaji per departemen\n",
    "df.groupBy('Department').sum('Salary').withColumnRenamed(\"sum(Salary)\", \"TotalGaji\").show()\n",
    "\n",
    "# 6. Menghitung jumlah karyawan per departemen\n",
    "df.groupBy('Department').count().withColumnRenamed(\"count\", \"JumlahKaryawan\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8062e32-8d65-4f5d-9c0e-90d0e6b63477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+--------------+\n",
      "|EmployeeName|Department|Salary|SalaryAfterTax|\n",
      "+------------+----------+------+--------------+\n",
      "|       James|     Sales|  3000|        2850.0|\n",
      "|     Michael|     Sales|  4600|        4370.0|\n",
      "|      Robert|     Sales|  4100|        3895.0|\n",
      "|     Jamilah|Accounting|  3900|        3705.0|\n",
      "|      Michel|Accounting|  4600|        4370.0|\n",
      "|       Assep| Enggineer|  5000|        4750.0|\n",
      "|       Maria|   Finance|  3000|        2850.0|\n",
      "+------------+----------+------+--------------+\n",
      "\n",
      "+------------+----------+------+------+\n",
      "|EmployeeName|Department|Salary|Status|\n",
      "+------------+----------+------+------+\n",
      "|       James|     Sales|  3000|Normal|\n",
      "|     Michael|     Sales|  4600|  High|\n",
      "|      Robert|     Sales|  4100|  High|\n",
      "|     Jamilah|Accounting|  3900|Normal|\n",
      "|      Michel|Accounting|  4600|  High|\n",
      "|       Assep| Enggineer|  5000|  High|\n",
      "|       Maria|   Finance|  3000|Normal|\n",
      "+------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Membuat kolom baru yang berisi gaji setelah dipotong pajak 5%\n",
    "df.withColumn('SalaryAfterTax', df['Salary'] * 0.95).show()\n",
    "\n",
    "# 2. Menambahkan kolom status berdasarkan kondisi gaji\n",
    "df.withColumn('Status', F.when(df['Salary'] > 4000, 'High').otherwise('Normal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9386e915-04a3-41dc-8b01-df14ea8d15f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+----+\n",
      "|EmployeeName|Department|Salary|Rank|\n",
      "+------------+----------+------+----+\n",
      "|     Jamilah|Accounting|  3900|   1|\n",
      "|      Michel|Accounting|  4600|   2|\n",
      "|       Assep| Enggineer|  5000|   1|\n",
      "|       Maria|   Finance|  3000|   1|\n",
      "|       James|     Sales|  3000|   1|\n",
      "|      Robert|     Sales|  4100|   2|\n",
      "|     Michael|     Sales|  4600|   3|\n",
      "+------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Penggunaan window functions\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "windowSpec = Window.partitionBy('Department').orderBy('Salary')\n",
    "df.withColumn('Rank', F.rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061d7f6e-0755-4246-9932-9408ad20c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-------------+------------+\n",
      "|anxiety_level|depression|sleep_quality|stress_level|\n",
      "+-------------+----------+-------------+------------+\n",
      "|14           |11        |2            |1           |\n",
      "|15           |15        |1            |2           |\n",
      "|12           |14        |2            |1           |\n",
      "|16           |15        |1            |2           |\n",
      "|16           |7         |5            |1           |\n",
      "|20           |21        |1            |2           |\n",
      "|4            |6         |4            |0           |\n",
      "|17           |22        |1            |2           |\n",
      "|13           |12        |2            |1           |\n",
      "|6            |27        |1            |1           |\n",
      "+-------------+----------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "+-------------+----------+-------------+------------+\n",
      "|anxiety_level|depression|sleep_quality|stress_level|\n",
      "+-------------+----------+-------------+------------+\n",
      "+-------------+----------+-------------+------------+\n",
      "\n",
      "+-------------+-------------------+\n",
      "|sleep_quality|          AvgStress|\n",
      "+-------------+-------------------+\n",
      "|            0| 0.9705882352941176|\n",
      "|            1| 1.9329268292682926|\n",
      "|            2| 0.9574468085106383|\n",
      "|            3| 0.9882352941176471|\n",
      "|            4|0.18617021276595744|\n",
      "|            5|0.23958333333333334|\n",
      "+-------------+-------------------+\n",
      "\n",
      "+-------------+-------------+----------+------------+----+\n",
      "|sleep_quality|anxiety_level|depression|stress_level|Rank|\n",
      "+-------------+-------------+----------+------------+----+\n",
      "|0            |6            |1         |2           |1   |\n",
      "|0            |13           |21        |2           |1   |\n",
      "|0            |1            |1         |2           |1   |\n",
      "|0            |18           |11        |2           |1   |\n",
      "|0            |14           |10        |2           |1   |\n",
      "|0            |10           |15        |2           |1   |\n",
      "|0            |15           |20        |2           |1   |\n",
      "|0            |1            |21        |2           |1   |\n",
      "|0            |14           |24        |2           |1   |\n",
      "|0            |11           |26        |2           |1   |\n",
      "+-------------+-------------+----------+------------+----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Buat SparkSession\n",
    "spark = SparkSession.builder.appName(\"StressLevelAnalysis\").getOrCreate()\n",
    "\n",
    "# 2. Load dataset\n",
    "df = spark.read.csv(\"StressLevelDataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 3. Tampilkan hanya beberapa kolom penting\n",
    "df.select(\"anxiety_level\", \"depression\", \"sleep_quality\", \"stress_level\") \\\n",
    "  .show(10, truncate=False)\n",
    "\n",
    "# 4. Filter: data dengan stress_level > 3 (tampilkan kolom penting saja)\n",
    "df.filter(df[\"stress_level\"] > 3) \\\n",
    "  .select(\"anxiety_level\", \"depression\", \"sleep_quality\", \"stress_level\") \\\n",
    "  .show(10, truncate=False)\n",
    "\n",
    "# 5. Agregasi: rata-rata stress_level berdasarkan sleep_quality\n",
    "df.groupBy(\"sleep_quality\") \\\n",
    "  .agg(F.avg(\"stress_level\").alias(\"AvgStress\")) \\\n",
    "  .orderBy(\"sleep_quality\") \\\n",
    "  .show()\n",
    "\n",
    "# 6. Window function: ranking stress_level dalam tiap sleep_quality\n",
    "windowSpec = Window.partitionBy(\"sleep_quality\").orderBy(df[\"stress_level\"].desc())\n",
    "df.withColumn(\"Rank\", F.rank().over(windowSpec)) \\\n",
    "  .select(\"sleep_quality\", \"anxiety_level\", \"depression\", \"stress_level\", \"Rank\") \\\n",
    "  .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddc4d3-1fa9-43ec-84c1-6b00e9bf5c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
